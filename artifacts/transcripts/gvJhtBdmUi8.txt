foreign good evening uh welcome to the newest edition of Dev day uh Dev day is a it's a monthly actually we have more than one talk uh a month in a few months now um is an event organized and conducted by sahaj we have a variety of topics in Dev day including um research and development topics on data science a artificial intelligence machine learning as well as a variety of topics in platform engineering data engineering and whatnot uh so today uh I'm gonna be the speaker my name is karthika vijayan uh and I will be talking about uh one of the research activities that we have conducted at sahaj as part of our as part of an initiative called sahaja labs and this uh edition of Dev day uh we have this topic related to natural language processing so I will start now I hope everyone can see my screen yes thank you yeah so the title of today's topic is multilingual information extraction from short text messages right and um I will just uh do a little intro of of myself my name is karthika Vijay I am a solution consultant with sahaja AI and I predominantly work in data science um my area of expertise are speech and audio processing and natural language processing I have also I have my bachelors and masters education are in electronics and communication engineering and I have a PhD in speech processing and applications and today I am going to talk a very specific Topic in natural language processing and in a multilingual scenario so the flow of today's talk is uh I will just do a quick introduction to multilingual NLP NLP being natural language processing I hope everyone also already knows that then we will do a quick recap of language representation models and how can they be used in multilingual setting multilingual app applications uh before talking before delving into the actual core of today's talk which are conversational assistant or information extraction for conversational system I will briefly touch upon uh and initial experimentation that they have conducted uh in multilingual uh NLP which is text categorization which which kind of led us to this uh which which was the predecessor experiment that led us to this information extraction experiment then I'll conclude so uh let's do an intro okay uh what is NLP to begin with right NLP uh intends to make a computer understand a language and uh when I say understand the language there are a lot of rules uh which are associated with each language right so there is a Syntax for the language there is semantics there are grammar associated with each language uh there is there are different sentence constructs which are uh ruling a language construct uh then there are contexts in which a word can appear and if it cannot appear Etc so when I when NLP intends to make a computer understand or learn a language by multilingual NLP of course it is a general extension we want an AI or a computer to understand multiple languages now the challenges are multifold because different languages are Guided by different rules and different settings so now we want one system or one pipeline to understand and learn um multiple languages so it possess multiple challenges there are of course similarities between different languages that we can leverage upon to make this multi-ringual NLP possible but of course the dissimilarities or the differences between languages also makes it a little bit of a challenge to add on to the set of challenges we know that when we converse when we type uh in social media or an email we generally do not actually constrain ourselves to this very strict set of rules right so uh there is a conversational style of of con of communication that will come into text messages as well so this again goes uh contradicts with the rules of that particular language making uh the natural language processing much difficult uh and then again uh we have Regional accented uh pods coming into when uh into our language when we converse when we talk when we text uh for example uh we we generally use uh Hindi words or or your mother tongue words in between uh English this results in a situation called code switching then there are uh very particular situations like Regional uh Singapore in English where this um uh vertical it's very funny uh you know the people uh generally type La in at the end of the end of every sentence so those are the regional constructs that creeps into uh a pure language scenario so all these process challenges uh in multilingual and LP whatever be the task or whatever be the application that you are looking into um so an ideal setting to make multilingual NLP possible or realize is we built a language agnostic system what I mean by a language agnostic system is uh one system that can realize NLP tasks without having to go through any language specific steps for example uh in the figure shown if I am building a text categorization system for taking in strings of text and putting into right categories uh like Maharashtra CM resigns to be in the class of politics or it's in the political news category uh as opposed to to this Hindi string where where it talks about Virat Kohli scoring a century it has to go into the sports category or the Malayalam sentence which is talking about um film Awards State Cinema Awards I would like that sentence to go into the entertainment category and I want one system I am building just one text categorization system to take in inputs from all these languages and still uh deliver the desired uh desired functions if these are functionality so this is a language agnostic system so ideally if we can develop a pipeline that can be utilized for multiple languages without any additional processing without any language processing steps that would be the best case scenario for realizing multilingual natural language processing uh why multilingual NLP I think the answer is like it's self-explanatory individuals in multilingual societies carnivals in multiple languages India being a rich multilingual Society we have 22 scheduled languages in India and several unscheduled ones um I if you look at into the world right there are approximately 7100 languages being spoken around in the world so uh if you're building an AI if you're building an intelligent assistant the natural progression is of course you want that assistant to learn and understand multiple languages uh so if you if you look at the Legacy systems for a little bit traditionally what we used to do is uh we will have language specific models and language specific system catering to each language and then we will have to build multiple language specific systems to all uh to cater all languages under consideration but if we plug our attention to multilingual NLP if we have a pre-trained resources which we we will touch up in in a moment we to make that system work we generally do not require huge amount of language specific resources be that data be that labeling capacity Etc so multilingual NLP is beneficial in in multilingual settings in designing several applications like a conversational assistant uh like a customer care uh automated customer care system right if I'm complaining that I'm not getting my internet properly same thing I can complain in my mother tongue which is Malayalam uh and I would like that assistant to uh give me appropriate response in my own language similarly if you look at another application like a bank banking um scenario an automated assistant to assist you with the banking you can you would like that assistant to answer um your queries about a car loan or a home loan uh whatever the language you are conversing in automatic management of archives is another huge application like you can think about the media houses and the media houses in India publishes newspapers and other visual as well as audio media content in multiple Indian languages and if you want have one system to maintain the archives be the English newsbeat Hindi news beats or what not uh that is highly beneficial of course uh then code switching as I mentioned we have a general tendency to mix and match multiple languages right uh and if we have a sentiment analysis or a sentiment classifier or a social media analyzer which is multilingual NLP and that can take care of this code special sentences that would be ideal as well so this is why we should go for multilingual NLP rather than language specific resources uh having said that I would like to say say language specific models if they are properly built with proper amount of training data that will any day outperform multilingual uh systems because multilingual systems have to learn multiple languages where whereas as opposed to the language specific system which will just concentrate and learn one particular language how to perform a multilingual energy so we have visited the words and the wise now let's talk about how which is the most important thing right uh so I I would just quickly recap on the Legacy uh systems which are which generally uh use language specific modeling if you have five languages under your consideration for one particular application you have to actually build five different language models then put a language detector like a switch in in the front end to identify the language and throughout the queries to the corresponding language model for further processing as I already explained this is a a tedious talk you have to find the day find all the data from five languages you have to build your own language specific models and then also you have to build the language director which is in itself is not an easy task then another strategy which was like the widely widely used strategy even now people use it is build one language specific model preferably in English because date it is easier to find data it is easier to find pre-trained pipelines Etc and then used a translation so if you want an NLP to answer your query in Hindi first you transfer it into English do all the NLP based processing and finally one the answer in English comes out the output in English comes out translate it back into Hindi so this is again as I said it's a widely used strategy but the problem here is as you already you might have already seen Google translate right it's not ideal many times Google the the translation systems messes up uh that that the functionality it may destroy the sentence constructs it may destroy a valuable information Etc and there is an error which is attached to the translation strategy which will get added into your NLP pipeline which has an error error metric of itself so this will lead to error propagation again it's not an ideal strategy for multilingual natural language processing the third strategy is like the groundbreaking one which is transfer line I'm quite sure that many of you already know what transfer learning is but bear with me so the uh in the interest of the general audience I would just quickly touch up what transfer language so transfer learning is basically you have a model or a pipeline which you have trained with ample data resources ample labeling efforts for a particular task for a particular domain and for a particular input in our case let's just say for one language now you want to take this system and use it for another language for example let's imagine this model that is shown in the figure is a neural machine translation and I am looking to translate Hindi to English and now my interest or now my application wanted a Hindi to Marathi translation so I will try to tie up this whole pipeline first I'll I'll have a neural machine translation for Hindi to English and the next neural machine translation from English to Marathi and I will not actually Cascade two systems it will mostly a little bit more of an intelligent strategy of your reusing parameters reusing hidden layers intelligent reprocessing intelligent cross processing Etc so this kind of transfer learning is actually a part of the transactive transfer learning which is the transfer learning between feature spaces so initially you have assigned Xs and x t as the as the feature spaces they come from the same uh same settings same dimensional feature space now for your Target application the feature space itself has changed so this is a classic example of transfer learning and in our case this could be this is actually cross-link World translating a second a type of transfer learning is Imagine uh you have trained a system for text categorization or any any application for one particular type of articles in the same uh in the same language for example I want to tag medical articles published in English medical journals published in English now I want that same system to tag sports articles published in in in English itself so that's a domain difference Sports from medical terminology medical vocabulary to sports so this is a domain transfer and the third one is actually inductive transfer learning we will not go into detail it is like uh you have you have actually constructed a system for one particular application let's say topic categorization which we have already seen right now I can actually take the front end of that system uh plug remove the back end of the system plug it with another set of classifier another set of labels Point Unit and use it for sentiment classification so that is a task transfer now uh in in a general setting of multilingual NLP we have to plug and play with multiple of these uh cross-lingual transfer domain transfer and task transfer okay uh so this is General uh transfer learning now we can we have to talk about language representation models uh because um if you look at a NLP pipeline right uh this is the most important uh thing which is actually generating a numerical representation or mathematical representation of your language which is typed in text okay so what we are looking at is an embedding generator this could be most most commonly used it is going to be a Transformer architecture which has an encoder and ready code representation which is actually trained for one particular task most commonly Master language model what Mass language model does is it will take I have not going into the detail of Transformer architecture and multiple architectures because that is an NLP course in itself so let's talk about a Transformer architecture which is just an encoder and a decoder representation now um Google's birth is is the most famous and the most commonly used uh Transformer architecture language representation what it does is that while training now it it actually takes uh sees a huge huge amount of data billions of tokens millions of unique words uh in English of course uh and then um 15 of words are always mask it is it is just blanked out and the the the model the Transformer architecture is actually trained to predict the next word and it it will build upon it it will start with one sentence two sentences and finally it will see billions of tokens and it will train itself to uh generate an efficient representation for each word and it will look at all the neighboring words it's called positional encoding it will also see the context of that word from multiple occurrences in that huge huge database uh in a way in in other words we can say that it will actually learn the meaning of the word the context in in which that word can occur in English and it will give you a number or or a vector representation it's a mathematical representation so that's how it generates natural language representation model for a particular language right now what we can do is we can actually take this birth embedding which is just the encoder architecture check out the the post processing side there there is a a fully connected layer and one what not for the next word prediction now you can actually take that embedding extractor or encoder representation uh plug it with our classifier error couple of classifier layer and use it for a topic categorization in English and and multilingual natural language processing model for like a language representation model is a natural extension what you do is you take a birth architecture which already has this positional encoding which will look in into the both directions of neighbors around the world and it will see actually data from multiple multiple languages in fact multilingual bird released by Google in 2016 2018 uh have seen multiple uh a huge huge data set from uh 104 languages if I'm not wrong okay so what it does is it will just predict the next word and and learn that language while doing so so it will uh learn English first then Hindi then um all 100 languages so the a little bit of um uh what do you call it the the trick or uh a little bit of um fine tuning or conditioning that Google did is it took all the Wikipedia articles uh belonging to this 104 languages and while doing so uh what can it ensure is that there is some amount of parallelism between the Wikipedia articles right uh for example if you if you talk about a Sachin tendulum cricketer there will be a Wikipedia article attached to Sachin Tendulkar in English in Hindi in Marathi and even in other foreign languages uh so uh and a Wikipedia article about Sachin Tendulkar will talk mostly roughly about the same things how many centuries he he scored how many matches he played where is he born what's his birthday these things are going to get repeated in multiple languages so uh the bird multilingual bird sees individual languages and learn those languages and also it leverages for this some amount of parallelism that is ensured in the database so that it can actually make that associations one-to-one associations between English and Hindi and Hindi and Marathi so that while you use it for multilingual setting in the future it will still give you a good uh faithful language representations or word embeddings um uh Facebook actually took it too little further they use the cross-linkual pre-training also on top of a bird architecture their birth is uh called Roberta which is a robust but uh obviously trained bird and they used an additional step of cross-lingual pre-training and their language model has a language representation of 100 languages or much more data than Google's input uh embod is trained on Wikipedia articles whereas XLR is trained on common crowdsourced data it's crowd uh crawled data um so that is uh all about language representation model so basically what it does is it give you word embeddings which captures the meaning and context of a word uh from a large which it learned from a large large database now that we know that we have this uh this language representation model which can act as the core of any NLP pipeline let's talk a little bit about a language agnostic pipeline I am going not a pipeline can as can be as elaborate as you want depending upon your task or as simple as you want um it's mostly uh how you design it so I am going to talk about a very very simple uh pre-processing uh post processing and feature ISO pipeline uh because uh going to final details is not the intention of this talk right uh so in pre-processing um uh there is a tokenizer which is um always there which basically cuts a sentence into individual words um and since we are talking about uh most of the languages including most of the Indian languages are words a white space separated so we we put a space in between two words so a white spaced organizer is good enough for most of the indic languages that we are interested day and this is of course a language independent you just have to look for white spaces and cut long sentences into tokens and tokens being words in our case then think about the processing uh for example a classifier if if it is topic categorization that you're talking about it is a classifier in the back end right so uh what is the classifier work on it just works on embeddings or vector representations it doesn't care whether the language is English or Hindi or Marathi or Malayalam and what not it just takes in a vector set of vector representation sector vectors and classifies it uh depending upon if it is a supervised classifier depending upon the labels that you have provided so the only component that we have to focus on that we have to keep our attention on is the feature riser and we we have actually already seen uh the language representation model right this is a good enough feature right there are four multilingual setting because it actually have understood and learned uh 100 languages so we can actually use this uh to build our pipeline now there is a small Crux here okay uh even though we we talk about these language representation models as pre-trained resources um they have actually seen context and meanings and sentence constructs from the database that is provided to them while training uh in Google's code Google's case it is Wikipedia articles in Facebook Facebook's case it is uh crowdsourced data most probably from entities that Facebook owns right the Facebook uh post that we make Twitter or other crowdsources with data so now it has seen this huge database agree but if we want to actually Port this and use it in a particular dedicated application that that is of our interest for example a conversational assistant which will I own a business and I want a conversational assistant to assist with the sales uh then it is not guaranteed that this uh model this pre-trained models that are embed or xlmr has actually seen the context of the the the text uh or the language that is going to be coming in my case which is very application specific this is a classic example of domain uh mismatched or domain transfer that we have to uh think about so if I want uh in birth to work in English embed of course Works in English because it has it is being trained on English but if I want Amber to work in my particular business application which is a sales assist understand which can answer to queries regarding the product that I manufacture and sell out I have to actually show a little bit of data from my application to embed so that it can actually fine tune on top of that data and so it all of course leverages on already existing information or already existing knowledge in it and it also Tunes itself to cater to my business application which is very uh for uh which is uh which has a specific set of vocabulary which but may not have previously seen right so a fine tuning of any of these pre-trained models for the tasks we need for the domain we need and the code for the language we need is uh definitely definitely required this is again a non-flight I was just mentioning it um so before going to the conversational assistant and information extraction uh for the purpose of a conversational assistant I've just uh quickly touch upon one of the first experiments that we did which is topic categorization again copy categorization is not a very uh difficult problem because uh most of the cases in topic categorization it's like for example the news articles categorization we have ample amount of data at least a paragraph We which we need to uh decide whether it belongs to a particular category of like business or a politics or a medical article right so it's not a very difficult task so for our experiment we we took this indic NLP data set which is an open source data set out there uh which has news articles belonging to multiple multiple languages okay uh and categories of the news articles are uh something like business news political news uh uh news on politics Entertainment News sports news Etc and we need to classify uh the Articles into corresponding category so this is again a case of task adaptation remember that embed is actually trained for uh predicting the next word but we can actually take the embedding extractor which is the encoder representation and uh plug in our classifier at the back end of it and then just to fine tune it for a little bit of data and measure the accuracy or deploy for the particular application so of course the pre-processing is pretty much a very simple embedding extractor that we can use either Emperor or uh xlmr and finally we have plugged in classifier layers out at the at the back end of the system and uh we noted accuracy as our Matrix so now this experiment is again not that um it's not like the groundbreaking experiment people already know it so what we did was we took news articles in Telugu and fine-tuned xlmr with labels it is a labeled data set so I know which which all articles belong to business category I know which all articles belong to entertainment categories so on and so forth okay so what we observed was when we fine-tuned uh the xlmr with Telugu and tested it on a test case which is not part of the training data we got above 90 accuracy for all the languages that we experimented with Telugu online Malayalam in this case all of them got 90 classification accuracy or categorization accuracy which is not surprising we already know that xlmr has good uh has seen good enough uh data uh from each of these languages and we just did a domain adaptation we just asked it to actually see the context and see the association of these particular embeddings to the were the category levels so we have seen it we have shown it a little bit of training data which is used for fine tuning and we got a 90 accuracy no no issues here now uh what we are interested in is uh the the problem statement is going to take a little bit of a turn here excuse me um so the problem statement is going to get a little bit of a turn here imagine um you know a multilingual Society like India right well if we compare the amount of uh media visual audio slash text um uh media out there we could actually see that it is very skewed so a language like Hindi will definitely have more resources out available out there just because of the sheer volume of a population that speaks and Converse in Hindi in India as opposed to a language like Malayalam which is my mother which is being spoken in a smaller State called Kerala and it is its population is much less than the Hindi speaking population because Hindi is being spoken in multiple states in India right um and here we can talk about low resource languages so in a general context in a practical scenario there are definitely going to be high resource languages and low responsive languages so even for the purpose of fine tuning I I agree that all these models like xlmr uh embed Etc uh have seen actually enough uh General context data I would call it like General data because it's not very catered uh towards a particular application in hand like for example this my business application then even to fine-tune a model like that it is not necessary that I have all the data that is required to the appropriate amount of data that is required to fine tune these models available in all the languages that is of interest to me so if I can use uh still use fine tuning data from Hindi which is out there and it maybe it is easier to get labeled uh get a supervised training data set ready and use it only use that to train uh or fine tune this particular model and use it as a transfer learning for multiple other languages then this is the cross-lingual transfer that we have talked about earlier so we just did this a very uh basic experimentation we retrained or fine-tuned this particular Excel number based model with whose task was topic classification so we already did the task transfer already then we are now you doing a cross lingual transfer so we showed it or fine-tuned it uh xlmr based model with Malayalam data only to perform the task of topic categorization and we tested it on multiple other languages okay and um not this was like a little bit of surprise so Telugu and Gujarati gave above 90 accuracy uh just a notice again that the training or fine tuning is only done with Malayalam data and Telugu and Gujarati performed better 90 accuracy okay there is some similarity for linguistic similarities between Telugu and Malayalam um so that's kind of justifiable Gujarati is a little bit of a uh it's not very linguistically similar to Malayalam on the other hand Tamil is very very linguistically similar to Malayalam and still it gave only 43 percent okay 43 is still a good score because there is this is zero short Landing so basically our model of for this particular topic categorization has not been fine-tuned on Tamil data so if it is a general case Zero short learning 43 is still a good number and Punjabi which is linguist again not linguistically similar to Malayalam also gave 43 so now that now we have something to explain right so this is interpretation so that this is where the analysis and the research comes in um so two languages which are similar to Malayalam Telugu and Tamil gave very different results two languages which are very dissimilar to Malayalam Gujarati and Punjabi also gave very dissimilar results so uh linguistic similarity is not a constraint here it's not the contributing factor to this uh table this set of crystals in the table and uh but one would the linguistic similarity is the first thing that you will think of uh when you are doing cross-lingual transfer because um uh that is one uh one limitation or one sort of restriction that was put on transfer learning in the earlier days when it was Advantage when when the transfer learning was studied in between two languages there are multiple research papers which talks about how how to choose those pair of languages based on linguistic similarity predominantly based on linguistic similarities so that you can actually build on one-to-one associations between phrases between grammars and between general rules that governs the language Construction okay so this is a very uh difficult result to interpret because uh we kind of exhausted our options uh in terms of linguistic similarity right there then what we noticed was uh we of course did the enough data analysis what we noticed was Malayalam data in this case is had four classes uh business Sports entertainment and politics I may be uh I may be a little wrong here uh in the specifics but Malayalam data have four classes and we did the topic categorization and Tamil data even though it is linguistic linguistically similar to Malayalam data we found that it had an additional class I don't remember which one specifically uh probably it was politics which was not part of Malayalam data but the whole system was topic categorization right so there was a class mismatch our output class mismatch so that is one contributing factor but why right because our uh fronted which is xlmr is supposed to give you good good uh good language representation and uh we are actually fine-tuning with respect to uh class mismatch uh the classes so we did a little bit more digging and we uh actually found uh this notion of inherently parallel data how uh these models be it xlmr beat embed or in even birth in in case of English right it actually learns the meaning of a word meaning of a sentence it gives you sentence embedding that gives you word embeddings it gives you it learns from the context so um so this notion of inherently parallel data came up so I'll just explain what this so we all know about parallel data is this is this used to be one important constraint to uh build any neural machine transformation systems in the earlier days you you should have same same sentences in English and Hindi to build a system that can translate in English to Hindi okay so this is parallel sentences uh I would like to know the price of this car so this is uh this is uh a sort of a query a conversational assistant or a sales assistant will receive from a customer and similarly the exact same sentence sentence could be there in hippie so it's exact same sentences but in the pack in practical situation in real world right it is not necessary that we will find these kind of exactly parallel sentences but we would still find sentences with similar content right uh like for example I would like to know the price of this car in English which would roughly translate to I like this car what what would be the price of it so even though the sentence the words everything are dissimilar they are different uh the the content the meaning of these two sentences are similar it's it's more or less the same this is called what we call as inherently parallel data so and inherently parallel data is more common in real world than one would expect right articles on the same topic like news articles published by say media house on a particular Day in India so there would be a Malayalam edition of a newspaper an English edition of a newspaper as well as a Hindi Edition and the news articles would roughly be the same except for the regional page and and dedicated uh the kind of queries that are a chat board would receive for a particular application for a particular use case for example banking but people can come and ask what would be the interest rate for a car loan in multiple languages and that chatbot is expected to re receive similar queries only because it's not expected to cater to any other application so the news categorization experiment that I was talking about was conducted as part of this initiative called sahaja Labs where we literally sit and do r d and uh device solutioning uh for use cases uh here we have actually successfully used a transfer learning uh we fine-tuned uh xlmr model with malnuts data and actually obtained good accuracy in Tamil Telugu and Hindi provided when we actually conditioned the Tamil data and Hindi data to um belong to the same uh similar content we we restricted that our news data cannot go from uh you know from left to right end of the spectrum they they have to be similar content so then we actually assured the condition of inherent parallelism in our data uh that the The observed successful transfer learning or successful cross-lingual transfer of Knowledge from one language to another foreign provided that we used this pre-trained model called xlmr which actually have a scene language representation of all these languages under our concentration okay so this was the first experiment uh that we first not exactly first one of the first uh set of experiments that we conducted to um uh explore the extent of transfer learning that can be provided by these uh pre-trained massively multilingual models um right now we can talk about info information extraction from short text messages up until now the topic categorization experiment that we are talking about uh where actually on news articles so to make one decision um the classifier had a lot more data to look into because if there are 10 sentences in a paragraph there are 10 sentences of weddings that are uh 10 into x times word embedding now when we talk about short messages it's just one sentence uh so the problem becomes more trickier and short text messages is again a relevant thing when we are talking about conversational assistance text based conversational resistance um okay let's look at the study case conversational assistant for the sales support uh scenario is what we took as our study case this is an intelligent assistant which can understand the customer queries and answer intelligently uh provided the customer queries are um are related to one particular application for example if I am a business owner uh I and I sell washing machines and I have it a conversational assistant my expectation is that conversational assistant would receive queries only related to washing machines multiple type of electric machines people can come come up and ask what are the color options in this washing machine how much is the capacity of the washing machine uh Etc and uh so so on and so forth so this is a very dedicated uh assistant which is supposed to answer one particular set of customer queries related to a particular setting a particular application so uh stages in building a conversational assistant are basically we need to First receive the customer query and then we have to if once we receive the customer query we need to do an intent classification intent recognition right I would like to know what is the intent of that customer and the intent in this particular setting will be a closed set of contents it won't be an open thread so a user can come a customer can come and ask me about the color options of the washing machine uh the price of of the washing machine the um uh the capacity of the washing machine Etc right so there are only like so many intents that uh a customer can ask but training to this one particular application so intent classification what exactly is the customer asking for what is the intent of the customer what is the fine trying to find out is uh part of the intent classification now entity recognition is again another task so entity recognition and intent classification uh can be together known as information extraction for a particular conversational assistant entity can be closed reset or open set so a customer can come and ask me uh about one particular uh model of the washing machine so that model becomes an entity I would like to retrieve that information from the user message and so that I can uh the assistant can give the appropriate answer so this customer is asking about this particular model of the washing machine and he is also asking his intent is to find out the price of the washing machine so these two things clubbed together is called information extraction in point from the context of a conversational assistant so basically we have to understand the meaning of the query and we have to pick relevant details in the queries so that intelligent responses can be generated um and of course the next part is generate response to give it back to the customer so this whole study we used the rasa framework to implement and experiment with the uh with the chat board setting or conversational assistant now as I I'm discussing earlier right we did not experiment with pre-processing as well as post processing because those are known language dependent uh things so the focus was again on natural language understanding nlu uh and in this particular talk I will be only talking about intent classification for the conversational assistant use case nlu is again understanding the customer query understanding the user code for our particular uh experiment or research experiment we have uh showed we have received short user messages which are belonging to 13 different intents and we made sure that for the fine tuning or the training data I we have 150 user messages uh for each Intel so 150 times all these 13 interns are occurring in our data set now just to hint on the complexity of the task it is not very straightforward task as the news get a classification where we think where we discussed about four or five different categories here we had 13 interns and three different type of Interest four generic intents uh where a customer is coming and say hi I would like to know something uh bye I am done I got the information now I'm leaving so these are the kind of greeting and goodbye intents uh which are easier to classify then there are five individual interns five individual intents uh are a product details of worry about a product detail worry about a product price um color options Etc I think okay it's not here Etc and then there are four combination entrance okay combination times interns are trickier in a in a conversational assistant setting so a user is coming and US asking uh about a car for example about a car so he's uh trying to ask okay I'm interested in this particular car what would be the price of it and what are the loan options that are available so most of the auto mobile providers have their own Finance right so a user is coming and asking about loan as well as price and there are two intents in that same sentence so these are called combination intents which kind of increases the complexity of the problem and uh and the uh and the yeah complexity of the data set basically so it is very easier for a classifier to misclassify this combination intends to either a product price or a loan uh intent because those two intents are also there in our 13 interns right uh so this is the complexity of the data and labeled we have a label the test data around 50 user messages for each of these 13 interns uh process pipeline is again a straightforward we have a pre-processing embedding extraction which is uh we we actually compared M but and xlmr and then there is a classifier which is basically a couple of layers of fully connected layer and so much no then output we have we are actually tracking uh in terms of F1 score as well as accuracy in percentage and we used adaptation of massively multilingual models which which are basically xlmr and embed in multiple permutation combinations of of the languages under our concentration so to begin with what we did was we have embed and xlmr uh the new embed has 100 languages 96 was the first version and uh xlma has around 110 languages uh so what we did here is just a just to Showcase if we don't find you if we don't do that domain adaptation uh or and task adaptation What will what will happen so we took xlmr no fine tuning just tested it on Indian classification our results are around seven percent for English and Malayalam and then we used that fine tuning data to fine tune XLR Mark and this whole uh pipeline of classifier our accuracy in improved to 95 above 90 percent uh similar nature a similar uh behavior is obstacle for embed also and in a general sense people know that xlmr performs much better than M but particularly in conversational informal scenario just because of the fact that embert has seen more formal sort of data like in Wikipedia articles or as opposed to xlmr which has which has seen more informal conversational kind of data in Tweets in in Facebook post Etc because this is from common crawl it is the crowdsourced data uh so we focused early on uh xlmr from now on now what we did was we actually took Malayalam data uh because it was a personal choice I I am a Malayalam speaker I could uh perform the data labeling and data validation as well as I could perform the data validation for English what we did was we fine-tuned xlmr progressively with Malayalam data as well as English data because after data analysis we found that there are English code switching happening in in our customer queries as one could anticipate right because people are talking about names of multiple products a price of multiple products uh and terms like loan Emi uh Etc these things will not as a widespread you used in one's own native language right so we actually progressively fine-tuned xlmr based pipeline for intent classification using Malayalam data and English data uh now what we unsurprisingly English and Malayalam gave 90 uh above 90 accuracy oriented classification in this 13 interns now what we observed was it actually gave people performance in Hindi Tamil and Telugu also okay because uh we made sure that that inherent parallel nature of data is present because it's a dedicated customer queries received by a dedicated conversational assistant uh for sales okay so what we found was when we did no fine tuning with xlmr this is the data this is the accuracy around seven percent six percent six percent etc for multiple languages we just showed that model or a pipeline with xlmr some data in Malayalam and some data in English and our accuracies uh skyrocketed 293 uh of some 72 70 persons are also here in Tamil and Telugu but remember that it is zero short learning we didn't do any fine tuning any training with respect to Tamil as well as Telugu data and still we got faithful performance so this is the power of transfer learning and uh using massively multilingual models so massively multilingual models have already already learned all these languages in a general setting from the data that it has seen now we are doing task adaptation with respect to uh and also language cross-linkual adaptation with respect to one particular language and one particular task and it is able to successfully deliver the transfer learning capability to other languages provided that the task Remains the Same and the data domain Remains the Same so this is the result so let's wrap up the conclusions are pretty straightforward we are the massively multilingual models put out by Google Facebook they are they actually lifted a multilingual NLP from nothing to awesome uh and even with limited data available in some languages not all languages under our consideration uh with a minimum labeling effort with a minimum data cleaning and validation effort we can actually construct conversational assistance so I just presented the results of just one task in which information extraction we have actually similar results in the other tasks also uh so to actually build a multilingual conversational assistant we can use massively multilingual models fine-tuned with minimum uh efforts in terms of data preparation in terms of uh adaptation training Etc and uh we can actually port or transfer this knowledge uh across languages provided that the data domain and the task Remains the Same and we can think about multi-fold business applications right sales assistant conversational system for sales conversational system for what not all the kind of business uh that we encounter in our day-to-day life public address systems news archiving and news archiving has multiple of applications it can it will be the same in news it will be beneficial for research Vehicles which legal articles Etc so there are multi-fold business applications uh so thank you um I would like to thank my colleague ocean uh who was uh there with me uh throughout this uh entire research who is also part of sahaj labs we did this research together so I'm present again to have both of us so if you have any questions please reach out to us and also if you have any questions regarding today's topic shoot now foreign model so maybe uh so it means can we is there any possibility of developing uh Universal model which understands Suppose there are five languages all the mannerisms for those languages uh will be modeled in that particular model which all those indexes and semantics so this will definitely it will be a large Insight but it will give more accuracy then uh current set of art but and MLM kind of model so yeah so basically this massively multilingual models are one step towards that Universal modeling right but when you talk about Universal models of course you cannot model the universe uh so there are going to be some restrictions so for a particular application in one's own hand what are the restrictions that we can play around is actually the Crux so in our case we had this conversational assistant sales assistant in our mind uh and how can we make you make the most out of the existing resources is what we study and our study uncovered this uh whole notion of inherently parallel data because people generally overlooked that aspect right um when you're talking about conversational uh any application any nlps NLP applications with closed lingual transfer this is not what people are looking for but think about from a business application standpoint right this is more common than you ever imagined this is gonna be big huge yes yes right but I agree with you uh second question is regarding accuracy and so the accuracy results so don't you think uh those results are depends on training as well as test data yes of course I mean I wouldn't claim that it kind of solved all the problems in the universe of course us so if you see that 70 accuracies that I have reported in the last slide for Tamil and Telugu I am not a Tumblr Telugu speaker so I couldn't actually do the data validation that was required uh now that actually one of my colleagues helped us out with the Telugu data that 70 is also 88 now okay okay yes means uh Suppose there are classes of similar there is one class May spell there in Tamil and if classes are similar then there will be accuracy will be quite high in terms of the main one right yes okay okay uh yes uh yeah thanks for uh for this thank you thank you for your presentation yeah um I got thank you for the very nice talk hello I just wanted to ask like uh if uh like in the experiments did you also try like multilingual fine tuning on top of the like these uh massive multilingual models we did with a couple of languages uh that also gave similar results uh so I didn't put all the results today uh but we have actually faithful results for unseen languages also nice thank you okay uh if we don't have any more questions uh I am on LinkedIn my name is karthika uh please uh please shoot any questions any doubts uh if you want to collaborate uh if you are interested in sahaj please shoot any one of us a message also on LinkedIn I am also on LinkedIn uh please do keep a contact please uh attend our future editions of Dev day as well we have a variety variety of topics we announce it on Meetup and yeah see you all in the near future or if you are interested to give a talk you can connect with us as well yeah sure that too sorry missing sorry for missing that please if you are interested to talk about your work in any topic uh please bring us for that as well okay then we'll close the session thank you so much thank you thank you so much bye thank you bye
