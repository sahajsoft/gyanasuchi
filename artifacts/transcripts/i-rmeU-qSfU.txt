uh hey uh hey hi guys uh welcome to this dev day event uh in the on the online event uh so uh guys are you able to hear us properly on the zoom link yes yes can somebody please respond yes my chat is fine if somebody can speak that will help us to know if we are able to hear people yeah yeah we can hear you we are speaking yes loud and clear no you are able to hear okay so uh if we face problems in uh in audio uh you guys can type in in the chat and we will read it out okay yeah uh just a brief about this dev day so we uh organize this dev day monthly so it's a technical event we have we in the past we have done a lot of david is about different technical topics uh ranging from http disk based uh and how the disk works everything the today's daily topic is on kubernetes networking prashanth here is going to talk about that he can give you a brief about what is the topic and he can start on it so yeah welcome again i'll hand it over to prashant thank you am i audible so i can't hear you guys uh for whatever reason i don't know uh so probably you will have to put in it in the chat uh for for any questions or anything that you want to say to me okay so should we start yeah so let's start so one of the topic so in this topic essentially it came out because i have seen that people are using kubernetes in in the projects but then there are certain gaps that we don't really understand how how things essentially works and even you don't need to actually go to too deep uh to understand we are not going to talk about too much of like networking layers or anything like that but fundamental components that we use on day-to-day life how do they actually allow us to communicate with kubernetes and this once you understand this then you really understand if the setup of of your infrastructure that is done and how kubernetes actually is working with your cloud provider and allows you to expose your services and this will also help you in terms of debugging any issues or understanding or even designing that which services should be internal which services should be outside uh which services are part of which domain or which applications and all those things okay so that's where this the concept of this thing comes out we had some discussion around this uh internally and then we realized that there are gaps so instead of just doing it internal session we can say that we can do it as a as a day of day okay so this is the general idea the agenda that i'm going to actually go go on with so the pod communication so this is very basic in terms of that you have multiple parts how do i make sure that they communicate with each other so we kind of know that we can communicate with each other but how does that really work and if i need to do that how do i uh do it then there are uh the main concept in this is the service the kubernetes services that are supported and there are at least three of them that i am going to cover okay and then we can see that why do they need and what they are actually solving okay you don't need all of them you need to know which one you are going to use uh for a given particular problem or given situation then the next concept is the ingress and ingress and ingress controller okay so we are going to talk about what is ingress ingress resource what is the ingress controller and what is a controller in general and why do we need ingress if we already have something like a service okay so that's the overall uh agenda of this uh of this talk one thing about this is you will at the end of it once we cover everything uh you basically see the end-to-end picture so i'm so we will be able to access the kubernetes applications over the internet okay and you're not going to create a domain as such but then you can just add a domain and it runs the same way as long as you have access then you can attach it to a domain and then it essentially runs the same way okay so that's the general idea of the talk okay so we need some examples okay so i've just gone up and uh we just have two services simple things okay even if we can expose these two the same thing is there for the rest of the application that we have okay so we are going to go with two services we have just a customer service and a product service uh both of them have one endpoint called customers and the customer id so slash one is essentially the id and the products is is again the same thing you are asking for the product details so you essentially say product slash product slash one both of them are get requests so simple example uh but you uh you will see that we need to still do a lot of we will go through all this for a lot of concepts that are there in kubernetes okay so what we can do is let's start with a pod uh before that i will just show you my screen is my screen is visible on the on uh on the call as well right okay yes yeah so uh i have a repo online okay that if you want after this talk as well you can go and actually refer to this there are lots of things uh that i have done as part of it so prerequisite is essentially all the setup i already did okay uh like setting up the cube cutter setting up the cluster in kubernetes kubernetes cluster in aws i am using cops for that so all those commands and everything is in place if you want you can follow it later okay so this is where the session essentially starts okay so i am going to refer to most of these things we will go through all of this and essentially deploy all this on the kubernetes cluster so i will just show you the cluster right now if you see uh okay one thing i need to say here is so generally we use cube cutter correct uh i can say go with nodes i have created alias for that it is easier for me to do that so ktl is essentially cube cutter any command that you see here with ktl it is cube cutter so it is it is the font visible it's visible so you can see that we have a cluster that has like three master nodes a setup and we have two worker nodes that are running as part of it okay okay we uh right now i am using the default name space um so i can check whether there are any parts on this so right now there's nothing there okay no parts are there so we haven't created anything yet okay so now what we are going to do is as we said that in the okay let's do that okay so what we said is we'll start with deploying a board so simplest thing we can do is we can just see that can we deploy the customer service or one of the service as a pod okay so this is a simple bot definition that we are essentially using here okay so customer service is so we have created a container already so you can see that that is referring to an image called as customer service and that customer service is just a plain i think spring boot application which is running on port 8080 i can actually run the docker here if you guys want how do i get rid of this bar we know that sorry drag what okay so what i can essentially do is i can just run the pod locally so this is nothing to do with kubernetes i'm just running it on my local so what i'm doing here is i'm i'm actually saying that run this image okay this is the image stack and we are saying run this image and the only thing we are doing here is it's it's kind of a port mapping that allows me to access the application running we are going to revisit this uh the same command again okay so what we are saying is your port 8080 that is within the container is mapped on this port on my machine okay i can actually also do one more thing okay let's go with this okay so this is actually running a simple application and you can see that it is saying that i can access it on on this particular thing obviously i haven't mapped it to 8080 i have mapped it to 1 8080 so i have to change that but i can just go and do that so if i just open that up okay so customers1 okay so i am able to run this container locally and it is actually okay so it actually gives me a response back so i i am able to see the container details of the customer one okay now let's do the same thing or similar thing with a pod in this case we are saying that i want to create a pod okay in kubernetes the name of the pod is customers i have added a label called app with customers as the value and then i am defining containers so this is where i am defining a container i am saying that the name of the container is customers this is the image and the port in which the application is running that's the container port is 88 okay so 8080 is where the application is running so let's do the same thing so instead of running it on our local let's actually run this against the uh on the kubernetes cluster so you can say that right now there are no parts in the namespace so i can actually say give me a customers yeah so all i did is apply that yaml same definition okay and then let's see whether a pod is getting created for us so it is saying that it is already running so it actually goes through creation of the container and all that we can also see the details of that part so if i say customers whether he picked up the same details that we passed in it gives us tons of details to be honest but whatever we passed in we can actually check that so if we go up annotations we don't need to worry about that so we are saying that it's a pod okay so now i'm looking at the yaml inside the kubernetes quest so it is a pod and its name is customers so that's what when you do cube cattle uh get parts you will get this and in the end you are saying that these are the containers and this is the image that with which it is running this is the container port that we are passing so essentially picked up everything that that we have deployed okay great but how do i sorry it is deployed to it's on the cluster so that's why so if you say cube cutter i'm actually talking with the cluster okay you can also see uh that it is running on which node let's see that okay so if i say get par again okay and let's look at the details extra details we should be able to see which node it is running on is it status okay so this is the node it is running on okay so this is not my local machine you can see that i can show you that okay my local machine is 192 something something and this is a machine that is running in easy to uh it's a c2 instance we can actually go and check that hc2 instance okay so this is actually running on this machine okay okay so that's where this uh port is running this application is running great application is running but how do i do the same thing that i did locally i was able to access uh the the customer's endpoint but how do i do it now i have deployed okay so so what do we actually need when we want to actually communicate with an application what do you think what do we need which information we need to actually communicate with any network yeah so basically you need an ip and a port okay so if we have a ip of some kind and if we know which port we need to reach to then we should be able to communicate with that application okay so let's try to figure out what actually is the ip address of this pod okay so we already have this definition opened up post ip we saw that it is the ip address of the node okay but we also see something called as spot ip okay so it is saying that there is the ip address of this pod which is 137 in this case ending with 137 so what do i do how do i access this can i actually do anything with this port if i if i ping this ipad i don't think i will be able to do anything with this okay so this ip address is not within the same network that my laptop is okay it is in in the cloud on that aws and that too it is not even part of the aw it's part of the cluster itself okay so this part or this ip address is only resolvable within the cluster okay that means i should be able to communicate from another part of the cluster to this part okay so let's try to actually install some other parts so right now we have just one pawn correct so if i say so if i install one more part in this case i am installing a engine export okay so it is saying creating we don't really need that okay so now all both of them are up and running so now what we are trying to going to do is we are trying to actually access the customer endpoint from the nginx okay so because both of them are part of the same cluster i should be able to use the ip address and reach the power essentially okay so let's try to do that so how do i how do i access our send a request from nginx to to the to the other party customer spot so kubernetes actually gives you a way to execute random commands on the parts uh themselves so we can actually do that so i can actually say nginx i can say call whatever was the ip address of this guy so i'm using the ip address of that part i already know that the the service runs on port 8080 because that's the that's the container that's where the application is running okay or listening okay and then we know the url as well so that is the url so i should be able to actually get a response from it and you can see that i am actually getting a response let's actually make it a bit beautiful okay so i am able to get a response from the uh from this particular port so i am able to send a request from nginx pod to my pod on the pod ip okay and the port that part is uh the application is running inside that container okay now uh also so i uh i'm not going to show it as an example but just a footnote is the ip is at the pod level okay and if you see the pod definition pod can contain multiple containers okay that means if i have multiple containers in a single part all of them will have the same address within the cluster so same ip is there that also means that they are within the same it's like localhost they can communicate with each other on a localhost thing not available the pod level not at a container level okay so if you have three containers in a pod you basically have one ip for all three okay so that is good so we are able to actually access this but what about if i have multiple parts i cannot just go randomly and say that okay this is the ip and now send it i don't want to just run one part of an application right we never do that there are tons of reasons why we don't do that because we want to be able to essentially say that so in this case the reason i have said that auto recovery is not there because if i delete the pod that's it it's gone not gonna come back okay uh for that you are going to use a deployment that i will show you know in them in a bit then the next reasons are you need high availability so if my part dies at least there should be someone else some other part of the same application which is still up and running and serves the traffic okay so you basically need multiple parts for a application auto scaling is is essentially sharing the load i don't want like if i if i have like huge scale i am going to run multiple parts one part is not going to serve all the traffic this is not possible and then if i am rolling new versions of the application as part of that if i don't have multiple applications again there is a possibility that you end up with a downtime so all these are there are multiple reasons essentially but you are not going to just deploy one application uh deploy an application with a single pod and that's it you will end up with a situation in which you have multiple ips or multiple parts now the issue with this is so engine x what we did we sent a call request from nginx to a single pod ip okay but if i have multiple of them now how do i do that so if i have service one and service two service two has a multiple iop how do my service one actually communicates with it if it just sends it to one ip then rest of the parts are not really doing anything if you if i say that no you need to use all three of them then it's a headache for the service to do round robin or some kind of thing and then it also needs if you put in auto scaling in it then how does it know that which parts i need to communicate to if my port goes away and comes back up with a new one if i replace the part i might end up with a different ip so that is also not going to work so all kinds of issues are there parts are the ips are not going to be stable and i have multiple of them so what is the solution okay so what actually uh before we move on i just wanted to highlight this is kind of an important thing because this also uh will allow you to understand where we are when we are looking at the images in in this presentation so anything that is marked with blue essentially all these components are kubernetes components okay anything that is marked with with uh with a green are the aws components okay and anything that is marked with a orange is essentially a shared share component which is a mix of kubernetes as well as the aws so for in this case the example will be the the nodes so node is actually a ec2 instance but it is also node which is part of your kubernetes cluster so if i do cube catal get nodes okay so kubernetes does know about these all these nodes but at the same time this same nodes are also visible as you see two instances here so it is basically a kind of a shared uh resource in that case okay so just keep that mentally when you are looking at the images here so that that also means that anything and everything that we are talking about here is still within the cluster okay so we end up with multiple uh parts what we actually need is a single stable ip okay we don't want that ip to change okay frequently and we don't want to manage eyepiece so if part goes up if i scale out if i scale in if there are three parts now there are five parts later and then i scale in then there are two parts i don't want to actually keep on knowing how many parts ips are there and all that so all i need is a single ip that i can can connect to okay i don't need multiple of them and that too i want kind of a stable ip than a ip that is given by the part itself okay so now in this case again every component here is still within the cluster we are still within the cluster okay so that's exactly what your cluster ip service type actually does so kubernetes services we talked about kubernetes services coming i will show you how the service looks like and what it does so one of the type of this service is cluster ip this is the default type okay and we will see why it is called as cluster ip but you can get a guess essentially right now is it is giving you a ip a stable ip to communicate to in which you don't need to know about the underlying ips of the parts you don't need to actually worry about how many parts are running is this three parts five parts is scaling up scaling down you don't need to know that okay you will get a single ip to which you need to communicate to and that's it that's what you're doing okay so that's the cluster ip type let's actually check and see how does it look like so if i need to do that how will it actually look like so now we are creating a resource of type service so earlier we created a resource of type uh pod okay now we are creating a resource of service type again the name is kind of same we gave the customers as a name then here we are saying that so there is something called as a selector okay what it is doing is it is saying that i am an service okay which which will try to find out the parts that has this label okay that's what this selector is doing so that means if if we want our our parts to be served by this service we need to make sure that we our parts actually have uh this uh particular this particular label okay so before we go ahead let's actually create this scenario let's actually create okay what's the question where do i look at the question okay how do i go to that control again escape oh good can one node contains multiple different types of parts so the question is can a single node contains multiple types of parts the answer to that is yes so in case of kubernetes what you essentially do is you say that i need to run this parts and kubernetes is the one that tries to map the parts to the nodes okay so when we are defining a part we never say that we want to run it on a particular node you are not going to do that you can do some kind of selection okay if you want to do that but in general by default you never need to choose a a node okay kubernetes will assign a node that it thinks that is the correct fit for that that also means that one node can host tons of parts okay and if your pod or a deployment if your application has multiple parts they might end up with the same node they might end up on a different note if you want to make sure that they ends up on different node you have to make sure that you configure your services correctly okay so and close this okay so before moving to service let's actually try to recreate this uh this scenario in which we have multiple uh parts of the customer uh application okay so for that we have something called as a deployment okay let me actually close everything on okay all others okay so we have something called as a deployment so this is of kind deployment what it is doing is so if you see any resource in kubernetes these are these sections it has a api version it has a kind it has a metadata and a spec okay these are the main uh four things that will always be there uh metadata is mainly extra information about your resource that you're defining spec is where the main thing is main configuration okay so in this case what we are saying is we are saying that we want three replicas of something okay and it is saying that it is also giving a selector which says that the pods essentially which are under the which has this label as part of it are the ones that are managed by this deployment okay now it has something called as a template so this is more like a port template so instead of defining three so we are saying that we need three replicas of the pod that should look like this okay okay and then it starts with a metadata and a spec okay so it's like a mini sub uh resource definition okay so it has a metadata and a spec and this is kind of identical to what you saw in the pod okay so almost everything that you have here is is actually here okay all you are saying is i need a container called customers this is the image of that container and this is where the application runs in that container and then i have added this label this is the same level that is actually used by the match level so that's what the labels are being used to actually connect and manage the parts okay so let's try to actually update this so if i say get parts right now i have two parts okay i can get rid of that part you can say delete pod customers okay it's taking so long okay so now if i say get parts there is only one part running which is nginx now let's try to actually apply the definition of the deployment rather than a part okay so it is saying that it's created all of them are essentially running so now i have three copies of the of the customer part not one and because why we have three copies we have because we are asked for three copies we are saying that give us three replicas so obviously all three of these do have different ips you can see that that my parts essentially have different types okay so this is one probably is difficult to read but this is one uh record another record on the last record these are the three parts and this is the engine export all of them have different types so essentially we are having this situation okay and we don't want this situation we want a stable ip okay so three parts of the customer i can also show you the deployment is created so it is saying that i need to create three replicas that's what it is saying all of them are available okay now what we can do is we can now create this service that we are talking about so let's look at this service definition okay so all we are saying here is i want a service type it is called as customers it basically caters to all the parts that has app level of type customers okay and then i'm going to talk about the port but the main thing here is we are saying that it is of type plus typing okay so now if i create this service let me check underscore service okay so now let's see whether the service is created for us okay so you can see that service is created ignore this thing right now so service is created for us okay called customer let's see what it actually gives me so i can give i can say describe that service basically gives me more detail of what is happening there okay so as part of it what it is saying is it is of type cluster ip so which is expected because we are saying that we want a service of type cluster ip which we are saying here okay and it is saying it is giving us an ip called this this is a new ip that is given now if you see the parts okay this is not the ip of any of this part okay it's like a completely different ip okay so my service definition essentially is giving me a ip address okay so that is probably the stable ip that i am going to get okay so instead of three or four or ten ips i am now going to deal with one ip so again it should be easier to actually do this should we try to actually use this ip and try to access the customer okay let's do the same thing that we did so what we did was we executed we tried to access it from engine x okay so let's try to do that but in this time instead of a pod ip we are going to use the service ip okay so we are able to hit that end point with the service ip as well okay and on the same port that the pod is actually running on okay so great so at least within so right now you see the diagram again okay okay even if after the introduction of the service we are still within the cluster okay we are not able to access this service from outside we are still within the cluster everything is still blue okay so great so now the question is how does this service actually knows what are the part ips how does it do that so if you actually see here when you did a describe customers where is that question okay so there is a question in which is saying that does service ip change it is possible that it might change if you recreated the service okay uh it is less in a way you can say that it is more much more stabler than a pod ip okay so anyway it is an improvement okay but we are going to talk about something extra that makes it even even stabler okay so service ip can change if i delete a service and recreate it it is going to change but that also means that i end up with a with a downtime essentially my service is now down your and your business is affected because of that okay so it is less likely that you will recreate the service but yes it can change one more question checked uh selector mentioned in the service matches with the labels mentioned in the pod or labels mentioned in the deployment okay it is it is matching with the pod labels in this case all of them are same okay you can see that the the label on the app customers is what is the same label that is there on the deployment so that might be the confusion but it is not looking at this thing it is actually looking at this okay so it is the it is the pod label not the deployment level okay i can have different deployment label so the part label is what service is trying to figure out so selector is for the pod labels since we use cluster ip when we queried how did we know from which part the data was pulled we don't know okay so it might actually go to any of the three parts so the question that uh was asked on the on the chart was which part the the service the request actually went to okay so we don't know survey generally just do so it is actually doing a load balancing for us it does a round robin between the parts and sends the traffic so now it is actually distributing the traffic for us so we are getting an added benefit at that same time we are getting a single endpoint to actually communicate to which is much better so it's already a improvement okay so now we are talking about what how does it actually is able to figure out what are the part ips so if you see there there's something called an endpoint okay and the end point is again bunch of ips okay three to be exact in this case so where are these ip coming from any guesses ah these are the pod ips so if you see if you match this these are exactly the part ips so this is 140 is there then 113 38 will be there okay and 139 will be there as well and basically it is also saying that the container your container port is 88 so that's how it is actually able to figure out and this list is continuously updated as and when the parts are changed so if i delete a part the the ip addresses will be will be changed it also creates something called as an input so endpoint is just shown there okay but it actually creates an endpoint uh for us you can see that the endpoint record is created we didn't create this okay we just created the service but service created a endpoint that is named exactly as the service name okay so it is also named as customers okay and this endpoint is actually maintaining the ip addresses of the port okay so kubernetes ensures that any changes in the port because kubernetes itself is managing the parts or figuring out if the port dies and all those things so it kind of knows that part is changed or something is changed so it tries to keep up to date the endpoint list okay and those endpoint list is what this service is dependent on so service essentially sees the endpoint list and that's how it is routing the traffic to yeah what's the question oh it's within here okay can we specify algorithm for service to use different algorithm instead of round robin uh we might i don't know uh we can check so one of the good way of figuring out what all things you can do in general is is to actually use a cycle explain command in this case you can actually do a lot of things and you can see that what all things i can consider there are tons of things that you can configure you might or might not be able to choose an ip so in this case i think uh one of the reason you might want to choose is is like a stateful application uh or there is some state or like session or something and then you need a stickiness and then you configure generally you don't do that you try to avoid it you want to uh if you do that you are your load is not actually uniform okay so if i add two more containers if i have two containers if i had same number of containers essentially double the containers there is no guarantee in that case that i actually reduced the 50 of traffic or do that so it's not linear in that sense if you not do a round robin all all parts should be like same okay it doesn't matter how many of them if that is not the case if you need to know where where do i send the traffic then it is kind of a stateful service and then you need to deal it in a different way there is something called a stateful set that you can use we are not going to talk about that uh in this i want to keep it as simple as possible here okay but what i'm trying to say here is it's kind of a complete documentation you can see that cluster ip external ips and all tons of things and it also gives you an explanation of what it is uh and it will also give you links to the documentation sometimes it might have so like this or like this okay so it gives your documentation links as well and in a way because you are asking it within the context of of your setup it might be aligned to the to the version of the kubernetes that you're using so that is a good way to figure out what all i can do as part of the definitions of of these resources okay okay but i don't want to deal with ip addresses and ports what are we doing okay so no i patterson ports we don't want that okay so how do we actually get rid of this commonly we don't actually use ip addresses order even if that is how the internet is working we don't really say that instead of google we don't say some some number and then that's where you need to go okay so you essentially use a domain name of some kind and we never use a port as well because we have assigned a well-known port so well through the protocols so that way we don't need to deal with that okay so let's go with this uh at least the well-known port so we didn't do one thing here we just said that there is a port that is mentioned here okay and that in a way we just said that it is same as 8080 basically but that's not you need to do you can actually change it you can also see it here okay it is saying the port is 8080 that's what it is saying and target port is so there's something called a target potential target port it is saying that it's also 88 but we didn't say that okay so it figured it out the target port is 8080. so in this case what i'm going to do now is instead of uh instead of going with 8080 as a port i will go with the standard 80 port which is the http port so that way i can at least get rid of one component which is the port i don't need to worry about the port okay so in this case what i'm saying is my service will accept the traffic on port 80 and it will send it to send that traffic to port 8080 on the uh on the ports so quad will still still accept traffic on port 8080 but for service point of view i don't need to worry about 88 and all that so in in if i apply this thing now okay so i'm applying that same yaml this one so now if you say get services okay you should be able to now see that it is actually trying to listen on port 80 not 8080 okay so now if i do the same thing let's try to access the uh the customer's application from nginx pod and get rid of the 8080 thing because we don't need that so at least we got an improvement in the way okay so pod in the port is now gone what about the ip address okay i don't want to deal with the ip addresses as well that's where the kubernetes so kubernetes actually has a dns setup i can show you that as well uh i am not going to go into detail of how we can go it if if we have uh at the end of the session if people want to know about that we can do that but i don't think we will get time to cover that probably today okay there are tons of things that we want to cover okay so if you see this one uh if i say dns there is a internal dns within the kubernetes cluster so these are the dna to give me a ability to use the dns so that means instead of using ip addresses i should be you be able to use some when known names okay the way kubernetes services works is every service that i create okay i will get a corresponding the service name is used as the dns for that service okay so that means i should be able to use the service name which in this case is the customers okay so instead of ip address i should be able to say that send my traffic to customers okay slash customer slash one okay so that's what i should be able to do so this is a service now so instead of an ip address if i actually use the service name which is customers in this case then i should be able to access the same three parts okay or the customer application through within the uh within the cluster essentially so now it is much more even stabler than the ip address that you get from the kubernetes from the service itself okay so that is definitely a big improvement for us so instead of ip address we are saying we will use domain name instead of port we will say that your standard ports so no need to mention the ports okay yes yes service name is resolvable across the across the cluster not within the namespace so that's the that's the bonus section that i was going to talk about i'm not going to talk about that so what he's saying is uh that service names are only accessible within the namespace okay that is not the case you can access because the ips are at the kubernetes cluster level they are not specific to a namespace the domain names are also not specific to namespace but it's a short version that we use and we basically just avoid the name space and everything and that's why you don't need to mention that but you can access from one part that is running in one name space can access another port that is running in entirely different name source as long as it is within the same cluster okay okay so till now yeah you can have some service name between two names different namespaces by the definition of namespaces it allows you to do that okay i didn't want to go there we can cover that in the bonus thing what i want to try to do is here is i want to completely end to end okay if i just do some of this then you will not actually get everything okay so we will we can talk about the bonus section which is the dns how dns actually works but let's talk about that later okay there are tons of things that goes but it is possible and the name space allows you to use duplicate names that's the whole point of namespace so i can have the same service name in two different namespaces i can have same pod in two different names so same name part in two different names okay so all this all that is possible that's why the name spaces are there okay so still now we are in the blue zone everything within the cluster great you can communicate between the parts but what i need to cover again from outside okay so how do we do that okay how did we do it in case of the container we already did that right we run our container locally and we were able to access it as localhost or or we didn't use the ip address but we could use the ipad address of the of the of my laptop not the ipaddress of the container or anything else okay so i should be able to do that so here what we are doing is we are doing a port mapping kind of a thing i am saying that that the port 8080 should be mapped to port 1880 on my local machine when i actually execute this docker run so let's actually do that again so if i do docker run this service okay so it runs and now if i go to localhost 88 we already tried this okay let's go actually and use not localhost but my ip address that will also work okay so my ip address is 192.168.1.73 [Music] okay so this should also work because because of my setting it basically gave me that it is a http it's not a https request which is correct it's not secure essentially but i am able to access the pod contents okay on the ip address of of my local machine okay so if i if i do that that means now from a client point of view i don't need to know that you are running it as a container or it is as good as if you are running it on on this machine okay that means if i know the type if somebody else is there who knows ip address of my machine will be able to access it as if it is just an application running on my machine they don't need to know that it's actually not running on my machine running inside a container okay they don't need to know that if we can achieve the same thing then you can actually access it on the node okay now we are coming out from the pod or container world into onto the node side of things okay so if i if i am able to do that then i should be able to access the thing so how do i do the mapping okay so that's exactly what the node port type service is actually giving you as the name suggests it gives you a node portal it's a mapping on your node so it gives you a port on your node which is mapped to the container or the pod pod port okay so same thing that we did for the docker container we can do it uh by using the uh no port service okay so now see the color coding so everything here is kubernetes okay this is the shared resource because it's a node okay it is within the ec2 instance because it is within the aws as well as it is part of the kubernetes cluster because kubernetes understands need to run parts on that node so that's where the parts are running and client is something that is within the aws okay so we are now within aws not still entirely public but outside of of the kubernetes cluster okay so if we do this okay let's try to do this i'll get rid of this node port where is it okay is this visible zoom now yeah that's what i am trying to do is not doing it yeah but then nothing weird is going on command plus is not working for whatever reason let me know once it is visible this is better okay so now what we are saying here is so this part is all same okay port 80 grid services are accepting traffic on port 80 target board is same that's where we need to send the traffic to this type of change now instead of cluster ip we are saying i want a node port service service of type node port and at the same time we have added something called as a node port uh which is basically so we chose a port which is uh 3001 okay what is it there is a question what is the question is no port service part of aws or kubernetes this service definition is part of the kubernetes okay but the port on which the port is essentially opened up on these on the ec2 instance so instance is part of aws that's why i showed it as shared resource but the survey definition the service we are creating is within the kubernetes cluster okay it does that clarify okay i think somebody already answered that to be honest okay so let's see the services we are still have cluster ip service okay we still have cluster rp let's change that so if i say apply and instead of this i say node port service okay so something has happened okay so now it is saying it is a of type node port and if you see here it was just saying that i am accepting traffic on port 80 that's all it is saying but now there's something different here it is saying that it is port 80 which is mapped to port 3001 and uh and all this uh things we can also see some more details but that's the only thing that is actually changed so it did a mapping of of the port 80 of the service now we are not mapping the port ip we are essentially saying that service port 80 is mapped to 3001. okay pod port is not mapped container port is not valid that's just the mapping between the service port okay on the ec2 instance what that means is now if i use the easy to instance ip address and send my request to 3001 for the customer i should be able to reach to the pod whichever part that service is actually uh serving okay so any customer part my request will end up with but i should be able to do that i should be able to use the ip address of the ec2 instance send the traffic on this port and and i should be able to reach the port okay so let's try that for that i need to be inside the aws uh vpc okay i cannot do so that ip address so even if you get the ip address okay let's let's also see one more thing okay that where this guy is running so let's let's say that which which node is a okay let's not do that let's get the nodes that's it again so these are the nodes uh i can actually get the ip addresses but this this is essentially the ip address of the node okay so if i choose one node this is master node so i am not going to choose that this is the node of type so this is the worker node i can choose this ip address if i if i basically try to ping this ip address or do anything with it it's obviously a private ip and i i purposefully made the setup in a way that is like real work kind of thing you don't want so i could have created a cluster that everything is a public ip but that's not what you are doing on your projects you don't want to do that you want your clusters to be private you don't want to open that up so this is not going to work because this is a private uh ip it's not going to work it's only going to work within within the ec2 ins within the ec2 within the aws vpc so that's why this is marked as green it's not public okay so that means i have to actually log in to so i have created a bastion i can log into the bastion yeah so this is the bastion now i do same thing if i do ping and the same ip it should tell me that i am able to ping to that thing tell me anything or is it doesn't work on the ec2 instances okay let's try to actually directly do what we are trying to do okay so what we said that we want to do curl okay on http so that's where the ported but we are going to use the port so in this case i need that same ip address of the node so this is the node ip address okay and i need the port which we used as a node port okay slash customers slash one so i should be able to get something out of it so you can see that i am able to reach to that pod okay one of the board but i am able to send my traffic to and this is the first time we are doing it out from outside the uh cluster not within the cluster okay so at least we got something there let's make it a bit beautiful even i don't want the progress bar as well so it is easier to see so we are able to essentially able to access the customer part from within the ec2 instances at list okay so now uh which is good uh let's go to now so it opens up yes so again there is a setting to not do that you can choose not to open it up on all the uh thing but then if you are using this setup i will not do that i will tell you in the next section that in that case you can actually do it okay but you can essentially say that i don't want to open up it everywhere just open it up on the notes where exactly that part is running okay and that that is possible so now again coming back we chose any random node right we didn't say that okay this node or that node there were two nodes two worker nodes and three master nodes we chose any of them and say that okay just try to send a request to it and it just worked okay we we try it we can try it for the other one and it will still work let's try that okay so if i have uh what is the other ip addresses okay okay so we chose this one i can choose this one and it will still work because that's the default behavior okay and that's kind of a downside as well there are two downsides but it also guarantees that it's going to work it's not going to fail you know it doesn't say that it's every node will know where to send the traffic to okay so that's what it is doing is able to do it actually across both the nodes that means now port mapping is not done only on one node it is done on all the nodes all the worker nodes on the on the cluster okay so that's what it is showing here so because these are shared components that's why they are colored like that this is a this is where we are trying to do this is the bastion node okay nothing to do with kubernetes cluster no kubernetes component is running so it's not compended we don't know this node okay so this is purely ec2 instance sitting outside of sitting in the same vpc but outside of the cluster so nothing to do with cluster so that's why it is green all these components are all blue okay so they are part of the cluster okay so this is f this is good that we at least are able to now access the service from the uh from the uh aws okay so what we got okay what is the relationship or kind of a similarity between the node port and the cluster ip type of service so in case of cluster ip all we got was the cluster ib okay crusher ip service in case of node port we got a cluster ip as well as a node port so if you see we actually also have oculus type let me get rid of this i'm coming i'm i'm back to my own machine now if i say get uh services you can see that i still even if it is part of even if it is type of node port i still have the same i still have the cluster idea assigned to it so that means it's still a cluster ip thing but something extra so in a way you can say that every node port is also a cluster ip service okay but it also has something extra which is the which is the node port mapped to all the all the nodes of the cluster so this is extra what we got so in a way it will still serve all the internal traffic with the standard ip and it will also have a dns record corresponding problem [Music] so i have a gateway exposed through the uh load balancer but the node themselves are not public they are still private ips okay but the load balancer that's sitting in front is actually public okay in production setup you don't do that you can avoid that uh doing that okay okay but we can't really rely on node ips right the same goes so i think we did the kind of a similar argument in case of pod ips we said i don't want to deal with pod if it's okay now we are saying that now deal with no difference it's like weird same same thing so you can't use node ips because which node see if i have like 10 nodes in my cluster or 100 node in my cluster i'm not going to deal with 100 ips and if i throw in like things like auto scaling and all that thing then it's like continuously changing we're going to track that and how do i send traffic uniformly on tons of things okay you don't want to treat your nose as a set of things like so female is essentially they come and go it's all dynamic you don't want to say that this is the machine where this service is running and this is where i sent and never touch this machine again that that's not the right approach for a cloud setup okay so and scaling scale out so all those are the reasons you don't want to deal with it did we saw the same thing earlier is it again same thing what can we do about this is it the same image did you saw this image earlier this is the same image right in this case it is pod so it's all it's all blue basically within the cluster but parts multiple parts stable ip now you are saying that we are within the uh we are within we are in a section that is like shared okay so nodes are there node ips are there multiple node ips and i still need a single ip so that i can communicate with it i don't want to deal with the node ips okay and your client can be public or aws depending on in this case okay so that's exactly what your next type of service does the load balancer service okay so load balancer service actually uh kind of allows you to do this it it gives you a stable so it creates a load balancer essentially uh as long as you are as long as your awa uh your cloud provider actually provides you the ability to do that so kubernetes can allow you to create a load balancer service so it allows you to create a load balance service which will give you a standard ip or a dns name okay and then you can use that to to communicate with the uh with the application okay so now let's try to uh see what a node ip cluster ip is okay okay good so now in this case what we are saying is this is of type the only thing we changed is actually it is of type load balancer that's the only thing we changed almost nothing exchanged but a lot of things will be actually happening as part of it so it actually creates a load balancer in aws we can we can apply this and then see okay what is the name of this load balancer underscore service okay get services will actually give you a service now there is something called as external ip that is sitting there which looks completely weird but you still have cluster ip still our let me get rid of this you still have a mapping of some kind which is still 3001 to 80 and a load balancer now let's check how what is actually doing here okay so if i if i go to the load balancer now okay so this is the load balancer it got created as part of the new service so that's why i showed it as a shared thing this service is actually created as part of a kubernetes resource but kubernetes is now creating aws so the cloud resource so that's why if you see the color is like shared thing okay so now what it is doing it created a load balancer so now if you go to listener okay listeners is where the similar thing what we said here we are saying that our input port is 80 okay the same thing on on the on the load balancer side is the listener in case we are saying that listener it is listening on port 80 and is sending traffic on on the instance port 3001 okay but which instances okay so if you say check the instances these are the two instances that are there so basically all our worker instances are here you can even open it up and you can check that out okay you can see the names here itself okay so these are the names of the instances okay so it is essentially what it is saying it send me the traffic on this dns name or ip address if you want to send it to ipad but dns is obviously better so if you send it to this dns on port 80 i will send this traffic to 3001 port on these one of these instances so almost similar to what you did for a service it's doing that from within the aws now again one more thing here is this guy has a it is actually internet facing okay because the way we have defined it is like public now again so is essentially an internet facing that means now i don't need to go to bastion or within the cluster or anything like that i should be able to directly heat my customer service from my local machine over the internet okay you can create a load balancer that is the internal then within the cluster within the aws but in this case we are just go with the default so now if i do call minuses hopefully i copied this is using the port 80 to listen so i don't need to specify any port okay i can just say customers slash one okay and to make it look good okay just formatted the json but now i am able to hit to my service from my local so now it is actually like you came out even from even outside of the uh aws infrastructure so great we did some we did achieve something but that 3001 is like hard-coded thing and so it's across all the nodes so if that port is not available or if it's a shared cluster this team is using that team is using the same cluster they might end up with the with with the service definition that exactly use the same port i don't want that okay and earlier that was useful because we were using that port in our urls but now that thing is completely hidden so i don't need to know about 3001. if you see the way we access this there is no 3001 here we just said send it to port 80 right http request goes to port 80. so i don't really want that to be a problem that means what i can actually do is i can actually get rid of this and let it map it to a dynamic port so that means if you go to the dynamic port okay so i got rid of the port association okay so what we can do is now just apply this to the cluster so it is even better now get services so now you can see that it is not a 3001 anymore it is mapped to some random port it ensures that the port is available across all the nodes more the nodes more problem is there in terms of figuring out the ports okay so this is essentially is required so now if i uh again from my from a client point of view then nothing changed because we are not even using that 3001 or so it changed i don't care okay i should still be able to access the service okay i am able to access the service you should be able to see this change in your uh listener configuration it is not there okay so you can see that in the listener configuration so all those things are changed in your cluster okay so that is also good which is a good thing now only thing that remains for this service at least is to add a dns which we are not going to do but you can map a dns point it to your load balancer and you are done basically you got the service exposed over the internet okay so this is what you got in a loader balancer you are getting two things changed a bit you got the dynamic port mapping which we use you can hard code it if you want to but we don't really need to so this is basically dynamic node port and you are getting a load balancer which is in our case it's internet facing you can go with the internal load balancer as well which allows you to access the this application from outside all great but i don't have one service okay i have multiple services okay we didn't even talk about product service but cube cutter able to create a aws resource like load balancer it's not cube cuddle it is creating it so the question is how cube cutter is able to create a aws load balancer so cube cutter is not creating aws load balancer it is the so we just created a service of type load balancer and our setup is essentially aware in terms of that it is dealing with the it is running on top of aws so any cloud that supports kubernetes type load balancer service will create a corresponding load balancer on those cloud services okay uh so that's that's where it is happening so it is kubernetes that is doing it not the cube cutter cube cutter just created a service of type load balance that's it okay so that is great uh but coming back to the same thing we didn't even talk about other services so we don't have just one service okay we have multiple services i want to expose them in a uniform way so let's first install the uh the product service so if i then do cube cutter apply so instead of this time i can go with product deployment i can show that it's almost same to what you have for the customer service okay the same thing almost same thing three replicas part template part is named as products uh the image is different but essentially the endpoint is different but like very simple service container port is same because the application is running on this input okay so everything say almost same as as the deployment for the customer service the same is for the product service so if once we have created this we should be able to see the parts so now you have three parts for the customers three parts for the product service okay great um so we want something like this okay what we want is some missing component uh again we are back inside the cluster everything is again blue we want something like this in which we send a request to some component it figures out that if it is a customer request it sends it to the customer and if it is a product request sends to the product but from client point of view it looks like as if i'm talking with one application two end points but one application i don't need to know that whether customer is implemented as a monolith as a microservice different parts same parts all those things okay so this is what we want to do and this is what is called as an ingress controller okay so ingress controller uh is a component that will allow us to do that so it does a path based routing uh for the parts so in general we can quickly talk about the kubernetes controller what is the controller so you have a resource and there is a corresponding control which is managed which is managing that resource okay so that means in terms of examples you have deployment resources will be a deployment controller corresponding controller you have a replica set we haven't talked about replica set but replica set is one of the resource uh which will have corresponding replica set controller okay your services so there will be a service controller so any type that you create there will be a controller that means by by this analogy we are talking about inverse controller that means there is something called as an ingress resource okay so there is a in place resource okay now generally ingress resource is like is like a plug-in to kubernetes cluster so most of the previous examples all the top examples are inbuilt in the kubernetes they are still independent component you can write another controllers of the inbuilt things so i can write my own service controller if i want to ingress control is something that you generally install or deploy okay so that means i already have done that so i can show you that get parts so if i say ingress engine x get rid of all this okay so i already deployed a part for the ingress control so the controller installation i'm not going to cover uh it it is there in the in that page if you want to follow that not this one so everything that i have done till now is actually here including pre-requisite including creation of the cluster everything okay it's a public repo you can go and check that i will share the link on the meetup page okay what the ins the installation of that ingress resource is already done okay install the ingress controller what we can do is uh we can actually create so controller is there that means we need to create ingress resources so we can create ingress resources for customer and the product service okay let's do that so means first let's see the services so we have two types of services actually we didn't create a service for for the product uh product service okay so let's create a product service okay so i can create a product service again nothing much there uh if you go and check the product service all it is doing is of type cluster right so we are back to cluster ib and is basically saying that accept traffic on port 80 send it to port 88 that's all it is doing okay so we have two services customers and the and the product service customer services load balancer we don't want that okay we don't need to have a load balancer customer we want to have it behind the ingress controller so i'm going to actually revert back to the customer cluster type well known in this case probably i will have to do a force because we have defined the node port so i'm going to get rid of that it gives you an error when i was trying it gives you error so in this case even if it ends up with the error it actually deletes and recreates the resource so now let's go and check so simple coming back to the cluster ips okay for customers and the uh and the products endpoint now let's talk about the ingress resources so for the customer ingress all we are saying is there is some host that i have defined this is the domain name you can give anything here right now whatever i have given is going to matter later on but at the current point you can give anything here as long as you send the request with this domain okay so what we are saying here we are saying that this is of type ingress okay we are creating a ingress rule okay so we are essentially creating a rule in this rule we are saying that any request that comes with this domain or this host and it has a path called customers as part of it send it to this service called customer on this port okay so we already have a service of of customer which is running on which is listening on this port okay so what we are saying is if we send a request to the in ingress controller part with this host and with this uh url we should be able to reach to the power to our part okay so let's actually do that if you see the ingress uh for the customer and the engage for the product it's all same okay just the because instead of customizating products so now what i will do is i will just create all of both of them okay so if i say give me the ingress things is actually showing me both the ingress uh that are that are created it's not going to work just yet so something is missing okay it will take some time to actually populate that hopefully it will die okay i haven't given up i have given okay we are going to talk about this later but now it is at least kind of ready okay so we have two ingress controllers one is called as customers one is called as products this is of type ingress okay now what we are going to do is we can actually so what we said is if we send it to this is what we said right if i send my traffic to the ingress pod switch to yeah but it has picked up the so it is going to work i don't know why it is showing classes thing but it will work because it picked up the changes so i can i can try to see why it is showing that let's try to see that i have given the class you can see the class is given things will change so if you are using another version or lower version you have to manage that but still now i have used everything lower than 118 so it should work if you're below 180 okay so let's try to actually send again from in uh okay one minute so we are now sending traffic to not our ports or even the service we're sending traffic to the ingress squad okay so for that we need the ingress pod and the ip address of that pod so if i say this this is the ip address of the part so we are still within the cluster okay so now if i send a traffic to this part to this ip address which port it is running on it is port 80 so i don't need to do anything okay i already have a rule that says that if it is slash customers and it's there but this is not gonna work because i am not using that post okay so for that what i need to do is i can i i can do two things i can actually add a host entry and say that this is the domain and and this is the ip address which goes to the pod or call actually gives you one more thing which is the uh which is to do a resolve so you can say that map this to this particular thing uh this ip map this force to this ip i can show you okay so you can essentially do this so in which what you are saying is let me zoom this up in which what you are saying is this is my domain okay i am going to use that but resolve this domain to this particular controller pod ip which is the ip address of the controller part so i can say that and then send a request of this thing which will send it to that so request will actually show that it is for that particular host but it will send to the right place okay so that means i should be able to actually use this thing that is clear right all i am doing is using the host ip uh or the the host domain but sending sending the traffic to the controller part okay so this is the ip address
